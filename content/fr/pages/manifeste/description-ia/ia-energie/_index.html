---
title: >-
  IA et Énergie
breadcrumb_title: >-
  IA et Énergie



url: "/manifeste/description-ia/ia-energie/"
slug: "ia-energie"
aliases:
  - /manifeste-technorealiste-sur-lia/pour-une-meilleure-description-de-lia/lia-frugale/
  - /manifeste/meilleure-description/frugale/
  - /manifeste/meilleure-description/ia-frugale/
  - /manifeste/meilleure-description/ia-energie/
date: 2023-10-02T16:14:51+02:00
lastmod: 2025-05-18T21:14:34+02:00
meta:
  hugo:
    permalink: "/manifeste/description-ia/ia-energie/"
    path: "/pages/manifeste/description-ia/ia-energie"
    file: "content/fr/pages/manifeste/description-ia/ia-energie/_index.html"
    slug: "ia-energie"
  dates:
    created_at: 2023-09-13T11:20:42+02:00
    updated_at: 2025-05-18T21:14:34+02:00
    published_at: 2023-10-02T16:14:51+02:00
search:
  id: "dc96208a-a651-4630-9ae0-5bd4ee557414"
  about_id: "0b3bf139-8ea3-42cb-a445-a9d44b258933"
  url: "/manifeste/description-ia/ia-energie/"
  kind: "Communication::Website::Page::Localization"
  lang: "fr"
  title: >-
    IA et Énergie
  summary: >-
    <p>L’IA, c’est du logiciel connecté, qui génère du code en plus. Dès lors, l’IA est synonyme de plus grande consommation électrique</p>
  body: >-
    <p>Remarque  <br>   <br>Nous sommes conscients, en nous attaquant à cette question, que notre propos est très situé dans un contexte mi-2023 et que la situation pourrait très vite évoluer.  Les IA sont loin d'être stabilisées dans leurs architectures  et dans leurs fonctionnements. 1 La plupart des discours sur l'IA se réfèrent implicitement à des systèmes centralisés et propriétaires. Or, on a vu que DALL.E et Midjourney qui fonctionnent effectivement selon ce modèle ont très vite été rattrapés et même dépassés sur certaines échelles de performance technique, par des concurrents open source pouvant fonctionner sur de simples ordinateurs personnels.  De même, ChatGPT, centralisé lui aussi,  open source en théorie, mais propriétaire dans les faits , est en passe d'être doublé par des concurrents véritablement open source pouvant être exécutés sur des machines aux capacités relativement limitées. Il est difficile de prévoir comment l'écosystème des IA open source pourrait évoluer, mais il me paraît probable qu'elles démodent assez rapidement la plupart des discours et des tentatives de régulation qui ne considèrent que ces systèmes centralisés.  <br>1  Elles ne le seront probablement jamais, puisque beaucoup de nouvelles applications peuvent être développées. On ignore aussi qu’il y ait une quelconque  main invisible  qui s’occupe de recoder toutes les vieilles applications pour les rendre conformes au nouveau standard. Les programmes logiciels s’additionnent le plus souvent, comme par sédimentation.</p>  <p> Croyances  <br>   <br>Il faudrait simplement  optimiser énergétiquement l'IA . Une affirmation aussi générale n’emporte aucune conviction sérieuse, puisque nous savons qu’IA, sans autre précision, ne signifie rien de particulier. Il faut donc nous intéresser à la réalité des logiciels et programmes pour envisager cette question. De manière générale, il faut bien comprendre qu’ un programme informatique, même lorsqu’on l’appelle  IA , ne s’endort pas lorsqu’il est fatigué  : autrement dit, i l n’arrête pas d’avoir besoin d’énergie supplémentaire , même lorsque ce qu’il fait est absurde, inutile ou dangereux. De manière générale :  c'est le changement d'état (1 vers 0 ou 0 vers 1) qui requiert de l'énergie . Une application qui est en veille ne coûte que l'énergie pour maintenir le système en état. Le coût minimum nécessaire pour un système en veille est l'écoute sur les nouveaux événements qui feront sortir le système de la veille. Sans cela, le système serait considéré comme étant à l'arrêt. </p>  <p> Histoire  <br>   <br>Le développement des IA aujourd’hui à la mode n’a certainement pas été frugal, leur emploi, et donc leur évolution actuelle ne l’est certainement pas non plus.  Cependant, on peut imaginer le développement d’autres IA, tout à fait frugales, qui viendront se rajouter, et pourquoi pas sait-on jamais, remplacer les premières… à moins que les premières soient tellement intégrées dans un système logiciel de production dense, qu’ elles ne disparaissent jamais . Autrement dit, pour un effort pour déployer une éventuelle IA, il faut un effort bien supérieur pour enlever ou modifier ce sur quoi, ou ce en regard de quoi on voudra l’installer.  On sait que ce phénomène d’accumulation est présent également concernant les sources d’énergie. Il est moins présent pour les objets techniques : achetant un rasoir électrique, je ne vais probablement plus m’acheter autant de rasoirs mécaniques.</p>  <p> Chiffres  <br>   <br>De manière générale 1 , un supercalculateur tel que Fugaku consomme 28 335 KW quand il fonctionne à plein régime. Une bonne carte graphique sans être sollicitée peut consommer 20x moins à l’heure que quand un jeu est lancé. Sur ces machines, on peut installer des programmes d’IA. Attention : dans ce qui suit, l’estimation ne concerne que la phase expérimentale et de la seule capacité de computation sur un GPU (unité de calcul pour ce genre de technologie.) Il ne s’agit en rien d’une dette carbone totale de l’IA, pas même pour ChatGPT : ce serait cette dette carbonée (entre autres GES) totale, qu'il serait intéressant de mettre en regard de l’éventuel retour sur investissement et bénéfices qualitatifs associés. 2 Voici une méthode possible, pas réellement simple, pour connaître la dette carbone totale de Chat GPT. 3  Il faudrait prendre une photographie de l’état des données de toute l’informatique mondiale connectée ; remonter toutes les séries d’inférences qui aboutissent à cet état, jusqu’à pouvoir isoler une série de 0 et 1 correspondant à la toute première implémentation de ChatGPT. Aucun chiffre n’est donné par l’éditeur, mais plusieurs ont tenté de l’approcher en l’estimant, et les chiffres sont édifiants : 0,00297 kWh par requête, 10 millions de requêtes par jour, soit une consommation journalière de 29 700 kWh pour les seules requêtes et leur traitement auxquelles s’ajoutent, 3000W par serveur pour son fonctionnement soit un multiple de 26280000 Wh par an et par serveur.  Sur la base des premiers développements de ChatGPT d’OpenAI, il est possible d’avoir l’estimation suivante pour les ressources de calcul fondé sur des cartes graphiques : Nombre de processeurs GPU : 1000. Consommation en Watts par GPU : 250W. Consommation totale du parc de GPU : 250 000 W. Consommation annuelle du parc avec un taux d’utilisation de 100% 24/24 7/7 base 365 jours an   : 250000 * 24 * 365 = 2,19 e+9 Wh an, soit 219 000 000 Wh annuels, soit 219 000 kWh an.  À titre de comparaison : 219 000 journées de jeu sur console de jeu vidéo, un peu plus d’un million d’heures de visionnage TV (ou 73 000 heures de VOD), 109 500 douches ou 54 750 bains, ou 54 750 lavages et séchages de linge, c’est aussi 219 000 heures de chauffage l’hiver, ou bien 109 500 heures de climatisation en pleine canicule estivale, ou encore 109 500 jours de travail avec un ordinateur fixe contre 328 500 jours de travail avec un ordinateur portable. Enfin, c’est 438 000 kilomètres parcourus en voiture smart électrique. C’est aussi la consommation électrique annuelle de 46 français. C’est aussi éclairer 851,64 m² du palais de l’Élysée (Hôtel d’Évreux uniquement) pendant 1 an. <br>1  Tout ce qui suit est une tentative d'objectivation/chiffrage totalement subjective, et éminemment contestable. Nous ne souhaitons pas ici proférer une vérité, mais montrer une manière possible de positionner le problème de façon technoréaliste. 2   https://www.hardware.fr/articles/955-10/consommation-efficacite-energetique.html 3  Elle paraîtra absurde, mais la méthode estimative nous paraît bien pire parce qu’ignorant les propagations à l’heure d’internet.</p>  <p> Problème  <br>   <br>Le problème est plus multidimensionnel qu’il n’y paraît.  Il est en fait à peine plus rationnel de chiffrer énergétiquement le bilan d’un service connecté à internet, que celui d’une loi qui entrerait en vigueur  : les dépendances sont trop nombreuses. Les frontières objectives pour borner l’analyse, inexistantes (voir nos autres développements sur la notion de  système ). Nous avons donné une idée de la consommation directe. L’usage induit aussi des  émissions dérivées  de la mise à l’échelle consommée par les fournisseurs de blocs essentiels à la publication et à l’utilisation de l’IA. Autrement dit, ChatGPT est un service connecté, et  ce à quoi il est connecté risque de devoir grandir avec lui . Il ne suffit donc pas de multiplier le nombre de requêtes, par le coût de la requête, pour avoir une idée de l’impact réel de ce service. Nous mettons en note, un billet de blog d’OpenAI, pour donner une idée de la machinerie  derrière le rideau .  [1] On voit qu’ il n’y a pas de sens de faire un  bilan carbone  comme pour un vulgaire scooter, ou une machine à laver. Alors que les frontières de l’IA sont purement conventionnelles, un tel bilan serait parfaitement farfelu : le “système” d’IA (qui est en fait un logiciel) est dynamique et capable de redimensionnement automatique ( autoscalable ), il fonctionne dans le cloud : c'est-à-dire que l’on peut (et souvent, on doit) programmer de la sorte que la fonction de calcul puisse être assurée, quelles que soient sa durée et sa dimensionnalité, quitte à élargir la ressource de calcul sous-jacente (et donc la puissance associée). De la sorte, l’ autoscalling  vient justement fournir les ressources manquantes, et l’IA ne se contente pas, comme un animal, de l’énergie à laquelle elle a accès. <br>1  Voici un exemple d’un billet de blog d’OpenAI concernant le fonctionnement de ChatGPT :  Our workload is bursty and unpredictable  : a line of research can go quickly from single-machine experimentation to needing 1,000 cores. For example, over a few weeks, one experiment went from an interactive phase on a single Titan X, to an experimental phase on 60 Titan Xs, to needing nearly 1600 AWS GPUs. Our cloud infrastructure thus needs to dynamically provision Kubernetes nodes. It’s easy to run Kubernetes nodes in Auto Scaling groups, but it’s harder to correctly manage the size of those groups. After a batch job is submitted, the cluster knows exactly what resources it needs, and should allocate those directly. (In contrast, AWS’s Scaling Policies will spin up new nodes piecemeal until resources are no longer exhausted, which can take multiple iterations.) Also, the cluster needs to drain nodes before terminating them to avoid losing in-flight jobs. It’s tempting to just use raw EC2 for big batch jobs, and indeed that’s where we started. However, the Kubernetes ecosystem adds quite a lot of value  : low-friction tooling, logging, monitoring, ability to manage physical nodes separately from the running instances, and the like. Making Kubernetes autoscale correctly was easier than rebuilding this ecosystem on raw EC2. We’re releasing kubernetes-ec2-autoscaler, a batch-optimized scaling manager for Kubernetes. It runs as a normal Pod on Kubernetes and requires only that your worker nodes are in Auto Scaling groups. https://openai.com/research/infrastructure-for-deep-learning</p>  <p> Conclusion  <br>   <br>Selon nous, l’IA (entendue comme l’ensemble des logiciels mobilisants des algorithmes d’IA, ou ayant requis la mobilisation de techniques de programmation typiques de l’IA) est  pantagruélique, et tel un trou noir, ne cesse jamais de croître  tant qu’elle est alimentée en électricité : le paramétrage de limitation de la consommation s’opère en local, ce qui n’a quasiment aucun effet sur les besoins globaux, s’agissant d’un système interconnecté branché.  Mais, des  optimisations  de la vitesse et de la consommation locales sont parfaitement envisageables : on connaît la modernisation des processeurs CPU vers GPU puis NPU. Cependant, aucune raison n'existe  de penser que ces améliorations locales provoqueront une baisse du besoin commun en énergie  (en valeur absolue), à cause de ce qu’est un système informatique intégré. L’IA ne se fatigue jamais de calculer  : tant qu’elle est branchée sur le réseau électrique mondial, il est à craindre qu’elle puisse brûler toute l’énergie du monde pour ce faire (l’IA n’est sensible ni au “signal prix”, ni aux craintes climatiques 1 ). Des logiciels non-IA feraient sans doute la même chose, peut-être moins vite. <br>1  Ce n’est d’ailleurs pas un hasard si Sam Altman, promoteur de ChatGPT, est aussi un acteur important de la recherche en matière de fusion nucléaire contrôlée (sérieusement envisagée comme l’énergie “du futur”) avec son autre entreprise nommée : Helion Energy.</p>

breadcrumbs:
  - title: "Technorealisme.org"
    path: "/"
  - title: "Manifeste"
    path: "/manifeste/"
  - title: "Pour une description de l'IA"
    path: "/manifeste/description-ia/"
  - title: "IA et Énergie"

design:
  full_width: false
  toc:
    present: true
    offcanvas: false


position: 7
weight: 7

translationKey: communication-website-page-0b3bf139-8ea3-42cb-a445-a9d44b258933



meta_description: >-
  IA et Énergie : L’IA, c’est du logiciel connecté, qui génère du code en plus, l’IA est donc synonyme de plus grande consommation électrique
summary: >-
  <p>L’IA, c’est du logiciel connecté, qui génère du code en plus. Dès lors, l’IA est synonyme de plus grande consommation électrique</p>
header_cta:


contents_reading_time:
  seconds: 509
  text: >-
    8 minutes
contents:
  - kind: block
    template: title
    title: >-
      Remarque
    slug: >-
      remarque
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Remarque
        heading: 2

    data:
      layout: classic


  - kind: block
    template: chapter


    ranks:
      base: 3
    top:
      active: false

    data:
      layout: no_background
      text: >-
        <p>Nous sommes conscients, en nous attaquant à cette question, que notre propos est très situé dans un contexte mi-2023 et que la situation pourrait très vite évoluer. <b>Les IA sont loin d'être stabilisées dans leurs architectures </b>et dans leurs fonctionnements.<sup>1</sup></p><p>La plupart des discours sur l'IA se réfèrent implicitement à des systèmes centralisés et propriétaires. Or, on a vu que DALL.E et Midjourney qui fonctionnent effectivement selon ce modèle ont très vite été rattrapés et même dépassés sur certaines échelles de performance technique, par des concurrents open source pouvant fonctionner sur de simples ordinateurs personnels. </p><p>De même, ChatGPT, centralisé lui aussi, <b>open source en théorie, mais propriétaire dans les faits</b>, est en passe d'être doublé par des concurrents véritablement open source pouvant être exécutés sur des machines aux capacités relativement limitées. Il est difficile de prévoir comment l'écosystème des IA open source pourrait évoluer, mais il me paraît probable qu'elles démodent assez rapidement la plupart des discours et des tentatives de régulation qui ne considèrent que ces systèmes centralisés. </p>
      notes: >-
        <p><sup>1</sup> Elles ne le seront probablement jamais, puisque beaucoup de nouvelles applications peuvent être développées. On ignore aussi qu’il y ait une quelconque <i>main invisible</i> qui s’occupe de recoder toutes les vieilles applications pour les rendre conformes au nouveau standard. Les programmes logiciels s’additionnent le plus souvent, comme par sédimentation.</p>





  - kind: block
    template: title
    title: >-
      Croyances
    slug: >-
      croyances
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Croyances
        heading: 2

    data:
      layout: classic


  - kind: block
    template: chapter


    ranks:
      base: 3
    top:
      active: false

    data:
      layout: no_background
      text: >-
        <p>Il faudrait simplement <b>optimiser énergétiquement l'IA</b>. Une affirmation aussi générale n’emporte aucune conviction sérieuse, puisque nous savons qu’IA, sans autre précision, ne signifie rien de particulier. Il faut donc nous intéresser à la réalité des logiciels et programmes pour envisager cette question.</p><p>De manière générale, il faut bien comprendre qu’<b>un programme informatique, même lorsqu’on l’appelle <i>IA</i>, ne s’endort pas lorsqu’il est fatigué</b> : autrement dit, i<b>l n’arrête pas d’avoir besoin d’énergie supplémentaire</b>, même lorsque ce qu’il fait est absurde, inutile ou dangereux.</p><p>De manière générale : <b>c'est le changement d'état (1 vers 0 ou 0 vers 1) qui requiert de l'énergie</b>. Une application qui est en veille ne coûte que l'énergie pour maintenir le système en état. Le coût minimum nécessaire pour un système en veille est l'écoute sur les nouveaux événements qui feront sortir le système de la veille. Sans cela, le système serait considéré comme étant à l'arrêt. </p>






  - kind: block
    template: title
    title: >-
      Histoire
    slug: >-
      histoire
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Histoire
        heading: 2

    data:
      layout: classic


  - kind: block
    template: chapter


    ranks:
      base: 3
    top:
      active: false

    data:
      layout: no_background
      text: >-
        <p>Le développement des IA aujourd’hui à la mode n’a certainement pas été frugal, leur emploi, et donc leur évolution actuelle ne l’est certainement pas non plus. </p><p>Cependant, on peut imaginer le développement d’autres IA, tout à fait frugales, qui viendront se rajouter, et pourquoi pas sait-on jamais, remplacer les premières… à moins que les premières soient tellement intégrées dans un système logiciel de production dense, qu’<b>elles ne disparaissent jamais</b>. Autrement dit, pour un effort pour déployer une éventuelle IA, il faut un effort bien supérieur pour enlever ou modifier ce sur quoi, ou ce en regard de quoi on voudra l’installer. </p><p>On sait que ce phénomène d’accumulation est présent également concernant les sources d’énergie. Il est moins présent pour les objets techniques : achetant un rasoir électrique, je ne vais probablement plus m’acheter autant de rasoirs mécaniques.</p>






  - kind: block
    template: title
    title: >-
      Chiffres
    slug: >-
      chiffres
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Chiffres
        heading: 2

    data:
      layout: classic


  - kind: block
    template: chapter


    ranks:
      base: 3
    top:
      active: false

    data:
      layout: no_background
      text: >-
        <p>De manière générale<sup>1</sup>, un supercalculateur tel que Fugaku consomme 28 335 KW quand il fonctionne à plein régime. Une bonne carte graphique sans être sollicitée peut consommer 20x moins à l’heure que quand un jeu est lancé. Sur ces machines, on peut installer des programmes d’IA.</p><p>Attention : dans ce qui suit, l’estimation ne concerne que la phase expérimentale et de la seule capacité de computation sur un GPU (unité de calcul pour ce genre de technologie.) Il ne s’agit en rien d’une dette carbone totale de l’IA, pas même pour ChatGPT : ce serait cette dette carbonée (entre autres GES) totale, qu'il serait intéressant de mettre en regard de l’éventuel retour sur investissement et bénéfices qualitatifs associés.<sup>2</sup></p><p>Voici une méthode possible, pas réellement simple, pour connaître la dette carbone totale de Chat GPT.<sup>3</sup> Il faudrait prendre une photographie de l’état des données de toute l’informatique mondiale connectée ; remonter toutes les séries d’inférences qui aboutissent à cet état, jusqu’à pouvoir isoler une série de 0 et 1 correspondant à la toute première implémentation de ChatGPT.</p><p>Aucun chiffre n’est donné par l’éditeur, mais plusieurs ont tenté de l’approcher en l’estimant, et les chiffres sont édifiants : 0,00297 kWh par requête, 10 millions de requêtes par jour, soit une consommation journalière de 29 700 kWh pour les seules requêtes et leur traitement auxquelles s’ajoutent, 3000W par serveur pour son fonctionnement soit un multiple de 26280000 Wh par an et par serveur. </p><p>Sur la base des premiers développements de ChatGPT d’OpenAI, il est possible d’avoir l’estimation suivante pour les ressources de calcul fondé sur des cartes graphiques : Nombre de processeurs GPU : 1000. Consommation en Watts par GPU : 250W. Consommation totale du parc de GPU : 250 000 W. Consommation annuelle du parc avec un taux d’utilisation de 100% 24/24 7/7 base 365 jours an   : 250000 * 24 * 365 = 2,19 e+9 Wh an, soit 219 000 000 Wh annuels, soit 219 000 kWh an. </p><p>À titre de comparaison : 219 000 journées de jeu sur console de jeu vidéo, un peu plus d’un million d’heures de visionnage TV (ou 73 000 heures de VOD), 109 500 douches ou 54 750 bains, ou 54 750 lavages et séchages de linge, c’est aussi 219 000 heures de chauffage l’hiver, ou bien 109 500 heures de climatisation en pleine canicule estivale, ou encore 109 500 jours de travail avec un ordinateur fixe contre 328 500 jours de travail avec un ordinateur portable. Enfin, c’est 438 000 kilomètres parcourus en voiture smart électrique. C’est aussi la consommation électrique annuelle de 46 français. C’est aussi éclairer 851,64 m² du palais de l’Élysée (Hôtel d’Évreux uniquement) pendant 1 an.</p>
      notes: >-
        <p><sup>1</sup> Tout ce qui suit est une tentative d'objectivation/chiffrage totalement subjective, et éminemment contestable. Nous ne souhaitons pas ici proférer une vérité, mais montrer une manière possible de positionner le problème de façon technoréaliste.</p><p><sup>2</sup> <a href="https://www.hardware.fr/articles/955-10/consommation-efficacite-energetique.html" target="_blank" rel="noreferrer">https://www.hardware.fr/articles/955-10/consommation-efficacite-energetique.html <span class="sr-only">(lien externe)</span></a></p><p><sup>3</sup> Elle paraîtra absurde, mais la méthode estimative nous paraît bien pire parce qu’ignorant les propagations à l’heure d’internet.</p>





  - kind: block
    template: title
    title: >-
      Problème
    slug: >-
      probleme
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Problème
        heading: 2

    data:
      layout: classic


  - kind: block
    template: chapter


    ranks:
      base: 3
    top:
      active: false

    data:
      layout: no_background
      text: >-
        <p>Le problème est plus multidimensionnel qu’il n’y paraît. <b>Il est en fait à peine plus rationnel de chiffrer énergétiquement le bilan d’un service connecté à internet, que celui d’une loi qui entrerait en vigueur </b>: les dépendances sont trop nombreuses. Les frontières objectives pour borner l’analyse, inexistantes (voir nos autres développements sur la notion de <i>système</i>).</p><p>Nous avons donné une idée de la consommation directe. L’usage induit aussi des <b>émissions dérivées </b>de la mise à l’échelle consommée par les fournisseurs de blocs essentiels à la publication et à l’utilisation de l’IA. Autrement dit, ChatGPT est un service connecté, et <b>ce à quoi il est connecté risque de devoir grandir avec lui</b>. Il ne suffit donc pas de multiplier le nombre de requêtes, par le coût de la requête, pour avoir une idée de l’impact réel de ce service. Nous mettons en note, un billet de blog d’OpenAI, pour donner une idée de la machinerie <i>derrière le rideau</i>. <sup>[1]</sup></p><p>On voit qu’<b>il n’y a pas de sens de faire un <i>bilan carbone</i></b> comme pour un vulgaire scooter, ou une machine à laver. Alors que les frontières de l’IA sont purement conventionnelles, un tel bilan serait parfaitement farfelu : le “système” d’IA (qui est en fait un logiciel) est dynamique et capable de redimensionnement automatique (<i>autoscalable</i>), il fonctionne dans le cloud : c'est-à-dire que l’on peut (et souvent, on doit) programmer de la sorte que la fonction de calcul puisse être assurée, quelles que soient sa durée et sa dimensionnalité, quitte à élargir la ressource de calcul sous-jacente (et donc la puissance associée). De la sorte, l’<i>autoscalling</i> vient justement fournir les ressources manquantes, et l’IA ne se contente pas, comme un animal, de l’énergie à laquelle elle a accès.</p>
      notes: >-
        <p><sup>1</sup> Voici un exemple d’un billet de blog d’OpenAI concernant le fonctionnement de ChatGPT : <i>Our workload is bursty and unpredictable  : a line of research can go quickly from single-machine experimentation to needing 1,000 cores. For example, over a few weeks, one experiment went from an interactive phase on a single Titan X, to an experimental phase on 60 Titan Xs, to needing nearly 1600 AWS GPUs. Our cloud infrastructure thus needs to dynamically provision Kubernetes nodes. It’s easy to run Kubernetes nodes in Auto Scaling groups, but it’s harder to correctly manage the size of those groups. After a batch job is submitted, the cluster knows exactly what resources it needs, and should allocate those directly. (In contrast, AWS’s Scaling Policies will spin up new nodes piecemeal until resources are no longer exhausted, which can take multiple iterations.) Also, the cluster needs to drain nodes before terminating them to avoid losing in-flight jobs. It’s tempting to just use raw EC2 for big batch jobs, and indeed that’s where we started. However, the Kubernetes ecosystem adds quite a lot of value  : low-friction tooling, logging, monitoring, ability to manage physical nodes separately from the running instances, and the like. Making Kubernetes autoscale correctly was easier than rebuilding this ecosystem on raw EC2. We’re releasing kubernetes-ec2-autoscaler, a batch-optimized scaling manager for Kubernetes. It runs as a normal Pod on Kubernetes and requires only that your worker nodes are in Auto Scaling groups.</i><br><a href="https://openai.com/research/infrastructure-for-deep-learning" target="_blank" rel="noreferrer">https://openai.com/research/infrastructure-for-deep-learning <span class="sr-only">(lien externe)</span></a></p>





  - kind: block
    template: title
    title: >-
      Conclusion
    slug: >-
      conclusion
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Conclusion
        heading: 2

    data:
      layout: classic


  - kind: block
    template: chapter


    ranks:
      base: 3
    top:
      active: false

    data:
      layout: no_background
      text: >-
        <p>Selon nous, l’IA (entendue comme l’ensemble des logiciels mobilisants des algorithmes d’IA, ou ayant requis la mobilisation de techniques de programmation typiques de l’IA) est <b>pantagruélique, et tel un trou noir, ne cesse jamais de croître </b>tant qu’elle est alimentée en électricité : le paramétrage de limitation de la consommation s’opère en local, ce qui n’a quasiment aucun effet sur les besoins globaux, s’agissant d’un système interconnecté branché. </p><p>Mais, des <b>optimisations </b>de la vitesse et de la consommation locales sont parfaitement envisageables : on connaît la modernisation des processeurs CPU vers GPU puis NPU. Cependant, aucune raison n'existe<b> de penser que ces améliorations locales provoqueront une baisse du besoin commun en énergie</b> (en valeur absolue), à cause de ce qu’est un système informatique intégré.</p><p><b>L’IA ne se fatigue jamais de calculer </b>: tant qu’elle est branchée sur le réseau électrique mondial, il est à craindre qu’elle puisse brûler toute l’énergie du monde pour ce faire (l’IA n’est sensible ni au “signal prix”, ni aux craintes climatiques<sup>1</sup>). Des logiciels non-IA feraient sans doute la même chose, peut-être moins vite.</p>
      notes: >-
        <p><sup>1</sup> Ce n’est d’ailleurs pas un hasard si Sam Altman, promoteur de ChatGPT, est aussi un acteur important de la recherche en matière de fusion nucléaire contrôlée (sérieusement envisagée comme l’énergie “du futur”) avec son autre entreprise nommée : Helion Energy.</p>






---
